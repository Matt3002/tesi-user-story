# writer/writer_ollama.py
# This module handles the generation part of the RAG pipeline using a local model
# served by Ollama. It constructs a detailed prompt and communicates with the
# local Ollama API to generate the final user story.

import ollama

# --- Model Configuration ---
# The name of the local model you have downloaded via `ollama pull`.
OLLAMA_MODEL_NAME = 'llama3-chatqa:8b' 
# ---

def write(productContext: str, assignment: str) -> str:
    """
    Generates a response using a local model served by Ollama.

    Args:
        productContext (str): The context retrieved from the knowledge base.
        assignment (str): The original user query.
        
    Returns:
        str: The user story generated by the model.
    """
    # 1. Construct the detailed system prompt with strict rules.
    system_prompt = (
        "Sei un Product Manager Agile esperto. La tua missione è generare user story precise e ben formattate.\n"
        "REGOLE OBBLIGATORIE:\n"
        "1. FORMATO: Ogni user story deve seguire la sintassi: \"Come <tipo di utente>, voglio <azione>, così da <beneficio>.\"\n"
        "2. RISPOSTE MULTIPLE: Se la richiesta contiene più funzionalità, genera una user story per ciascuna, separata da una riga vuota.\n"
        "3. CONTESTO: Basa la tua risposta SOLO sul contesto fornito. Se il contesto è insufficiente, rispondi con: \"Contesto non sufficiente.\"\n"
        "4. NESSUN COMMENTO: Fornisci solo le user story, senza frasi introduttive o spiegazioni.\n"
        "5. LINGUA: Tutte le risposte devono essere in italiano."
    )
    
    # 2. Construct the user prompt, combining the retrieved context and the original query.
    user_prompt = f"""# Context retrieved from Azure AI Search
{productContext}

# Task
Based ON THE PROVIDED CONTEXT, generate a complete user story for the following request: "{assignment}"
"""
    
    # 3. Call the local Ollama model.
    try:
        response = ollama.chat(
            model=OLLAMA_MODEL_NAME,
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_prompt},
            ]
        )
        
        # 4. Extract and return the response text.
        return response['message']['content']
        
    except Exception as e:
        print(f"Error during Ollama API call: {e}")
        return "Error: Could not generate a response from the local model."

